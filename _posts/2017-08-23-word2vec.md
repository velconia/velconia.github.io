---
layout: post
title: word2vec
categories: mlp
tags: ml
---

[Outline](http://www.cnblogs.com/iloveai/p/word2vec.html)

[Motivation](http://www.tensorfly.cn/tfdoc/tutorials/word2vec.html)

[One-hot representation](http://blog.csdn.net/u013362975/article/details/53319002)

[VSM](https://en.wikipedia.org/wiki/Vector_space_model#Definitions)
[Distributional hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis)

[Maximum Likelihood](http://www.cnblogs.com/liliu/archive/2010/11/22/1883702.html)
[N-gram Model](http://blog.csdn.net/baimafujinji/article/details/51281816)
P(Data|M) 能表示成 П (i: 1->m) f(xi|Θ)
计算量大, 不能表示词之间的关系, 且 n 从 1->3, 提升很大, 但之后提升很小
现在已经能计算n=10的结果了

平滑化: 当词的出现次数为0或全部的时候, 不能判定词的概率就是 0或者100%, 所以需要做平滑化;

[NN](http://www.cnblogs.com/subconscious/p/5058741.html)
[Active Function](https://www.zhihu.com/question/22334626)
[NNLM](http://blog.csdn.net/a635661820/article/details/44130285)
[All NNLM Implementation](http://www.flickering.cn/nlp/2015/03/%E6%88%91%E4%BB%AC%E6%98%AF%E8%BF%99%E6%A0%B7%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E7%9A%84-3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/)
[SimHash Algorithm](http://www.cnblogs.com/maybe2030/p/5203186.html)
[Word2Vec](http://www.nustm.cn/blog/index.php/archives/842)

Summary:
[Mine](http://wiki.baidu.com/pages/viewpage.action?pageId=372640030)
